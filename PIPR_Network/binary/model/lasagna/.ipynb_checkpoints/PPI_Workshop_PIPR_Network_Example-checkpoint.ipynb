{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPI Workshop : An example of Neural Network Training for Protein-Protein Interaction Prediction with PIPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <cite>[PIPR][1]</cite> Presentation : Multifaceted protein–protein interaction prediction based on Siamese residual RCNN \n",
    "\n",
    "Chen, Muhao, et al. \"Multifaceted protein–protein interaction prediction based on Siamese residual RCNN.\" Bioinformatics 35.14 (2019): i305-i314.\n",
    "\n",
    "- For our hands-on training, we will analyse an example of deep neural network, PIPR, that relies only on protein sequences as input to predict its interaction.\n",
    "- The original code from Chen, Muhao, et al. is available on github <cite>[here][2]</cite>. In this training, we have made some modifications, so that it can easily be understood and tested.\n",
    "\n",
    "[1]: http://dx.doi.org/10.1093/bioinformatics/btz328\n",
    "[2]: https://github.com/muhaochen/seq_ppi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Due to the variety of the users work environment (operating system, computational power), a certain number of choices have been made for the purpose of this training.\n",
    "- The global prerequisites are the following:\n",
    "```\n",
    "    python 3.6 or higher\n",
    "    Tensorflow 2.0 (with GPU support)\n",
    "    CuDNN\n",
    "    Keras 2.2.4\n",
    "```\n",
    "- Depending on the operating system, installing the GPU environment (tensorflow,Cuda,Keras) can vary. We propose the following links for step by step help on installing these elements, for <cite>[Linux][1]</cite> or for <cite>[Windows][2]</cite>. Note that using the tensorflow/keras environment does not necessarily imply having a GPU, you can proceed without the latest parts if not.\n",
    "\n",
    "- Our aim is, rather than just use an available prediction tool, to delve into the code to see how each step works. By analysing each part of the code, we wish to explain what important choices are made, and to which extent they influence the model's performance.\n",
    "\n",
    "[1]: https://www.pyimagesearch.com/2019/12/09/how-to-install-tensorflow-2-0-on-ubuntu/\n",
    "[2]: https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the code\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import sys\n",
    "if '../../../embeddings' not in sys.path:\n",
    "    sys.path.append('../../../embeddings')\n",
    "\n",
    "from seq2tensor import s2t\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM, Bidirectional, BatchNormalization, merge, add\n",
    "from keras.layers.core import Flatten, Reshape\n",
    "from keras.layers.merge import Concatenate, concatenate, subtract, multiply\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D\n",
    "\n",
    "from keras.optimizers import Adam,  RMSprop\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A certain number of libraries need to be imported for the network to be trained. In particular, tensorflow is an open-source platform dedicated for machine learning. Keras is a library enabling us to interact with tensorflow in a simple manner. Thanks to theses libraries, a variety of layers and transforms (such as convolutional layers, pooling layers, etc.) used in neural networks are directly available, without having to code them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(gpu_fraction=0.25):\n",
    "    '''Assume that you have 6GB of GPU memory and want to allocate ~2GB'''\n",
    "\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    #gpu_options = tf.compat.v1.GPUOptions(gpu_fraction)\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#KTF.set_session(get_session())\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#from keras.layers import Input, CuDNNGRU\n",
    "from keras.layers import Input, GRU\n",
    "#from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from numpy import linalg as LA\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most layers in neural networks rely on a high number of simple operations (essentially matrix multiplications), using a GPU rather than the CPU for computation is advised. GPUs are able to perform a high number of floating point operations, which is what is needed when training a network with a very high number of parameters.\n",
    "Using a GPU rather than a CPU is not mandatory, but using a CPU will make the most resource-intensive task of training the model much longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to input data and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to the protein sequences\n",
    "cur_path = os.path.abspath('')\n",
    "id2seq_file = os.path.relpath('..\\\\..\\\\..\\\\yeast\\\\preprocessed\\\\protein.dictionary.tsv', cur_path)\n",
    "\n",
    "id2index = {}\n",
    "seqs = []\n",
    "index = 0\n",
    "for line in open(id2seq_file):\n",
    "    line = line.strip().split('\\t')\n",
    "    id2index[line[0]] = index\n",
    "    seqs.append(line[1])\n",
    "    index += 1\n",
    "seq_array = []\n",
    "id2_aid = {}\n",
    "sid = 0\n",
    "\n",
    "seq_size = 2000\n",
    "emb_files = [os.path.relpath('..\\\\..\\\\..\\\\embeddings\\\\CTCoding_onehot.txt',cur_path), os.path.relpath('..\\\\..\\\\..\\\\embeddings\\\\string_vec5.txt',cur_path), os.path.relpath('..\\\\..\\\\..\\\\embeddings\\\\CTCoding_onehot.txt',cur_path), os.path.relpath('..\\\\..\\\\..\\\\embeddings\\\\vec5_CTC.txt',cur_path)]\n",
    "use_emb = 0\n",
    "hidden_dim = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next step is getting access to the input data. In our case, we only need the protein amino-acid sequences, which are located in the 'yeast' folder. Each line corresponds to the protein id, and its sequence. For example:  \n",
    "P00044,MTEFKAGSAKKGATLFKTRCLQCHTVEKGGPHKVGPNLHGIFGRHSGQAEGYSYTDANIKKNVLWDENNMSEYLTNPKKYIPGTKMAFGGLKKEKDRNDLITYLKKACE\n",
    "- If we want to train/test on another dataset, we will need to replace this file with our new dataset.\n",
    "- This step has been modified from the original code, so as to function for the Windows OS. In particular, paths are written differently between unix/windows.\n",
    "\n",
    "### Note\n",
    "- The dataset used here is the Yeast dataset for binary PPI prediction provided in <cite>[Guo et al. 2008.][1]</cite>. It is a dataset widely used for benchmarking, and contains a balanced number of positive and negative samples (11 188 interaction cases in total).\n",
    "- The quality and quantity of the dataset is essential in both training and testing the network.\n",
    "\n",
    "[1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2396404/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An epoch corresponds to one cycle through the full training dataset. If we stop the training after one epoch, the network will only see each protein once.\n",
    "- Typically, the more epochs are used, the better the network will be trained. In this code, the number of epochs should be at least 50-100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11188it [00:00, 386813.34it/s]\n",
      " 12%|█████████▌                                                                   | 312/2497 [00:00<00:00, 3097.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11187\n",
      "2 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2497/2497 [00:00<00:00, 3110.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11187/11187 [00:00<00:00, 2804140.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 11187/11187 [00:00<00:00, 3739077.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 10 13 15 17]\n",
      "{'0': 1, '1': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Labels for the dataset\n",
    "ds_file = os.path.relpath('..\\\\..\\\\..\\\\yeast\\\\preprocessed\\\\protein.actions.tsv',cur_path)\n",
    "#label_index = 2\n",
    "label_index = -1\n",
    "#Results file location\n",
    "rst_file = os.path.relpath('results\\\\15k_onehot_cnn.txt',cur_path)\n",
    "sid1_index = 0\n",
    "sid2_index = 1\n",
    "\n",
    "#Embedding and progress bars\n",
    "seq2t = s2t(emb_files[use_emb])\n",
    "\n",
    "max_data = -1\n",
    "limit_data = max_data > 0\n",
    "raw_data = []\n",
    "skip_head = True\n",
    "x = None\n",
    "count = 0\n",
    "\n",
    "for line in tqdm(open(ds_file)):\n",
    "    if skip_head:\n",
    "        skip_head = False\n",
    "        continue\n",
    "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
    "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
    "        continue\n",
    "    if id2_aid.get(line[sid1_index]) is None:\n",
    "        id2_aid[line[sid1_index]] = sid\n",
    "        sid += 1\n",
    "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
    "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
    "    if id2_aid.get(line[sid2_index]) is None:\n",
    "        id2_aid[line[sid2_index]] = sid\n",
    "        sid += 1\n",
    "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
    "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
    "    raw_data.append(line)\n",
    "    if limit_data:\n",
    "        count += 1\n",
    "        if count >= max_data:\n",
    "            break\n",
    "print (len(raw_data))\n",
    "\n",
    "\n",
    "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
    "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
    "max_m_seq = max(len_m_seq)\n",
    "print (avg_m_seq, max_m_seq)\n",
    "\n",
    "dim = seq2t.dim\n",
    "seq_tensor = np.array([seq2t.embed_normalized(line, seq_size) for line in tqdm(seq_array)])\n",
    "\n",
    "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
    "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
    "\n",
    "print(seq_index1[:10])\n",
    "\n",
    "class_map = {'0':1,'1':0}\n",
    "print(class_map)\n",
    "class_labels = np.zeros((len(raw_data), 2))\n",
    "for i in range(len(raw_data)):\n",
    "    class_labels[i][class_map[raw_data[i][label_index]]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For both the training and test sets, labels are needed to either make the network, or analyse its performance.\n",
    "- As for the protein sequences, the labels are in the 'yeast' folder. Each line contains the two protein ids, and wether they should interact or not. For example, 'Q08949 P38089 1' means that both proteins interact.\n",
    "- The result of the testing phase will be written in the 'results/' folder. Again, for this example, the paths were modified from the original code to work for Windows.\n",
    "- The rest of this code consists in checks for protein lenghts, number of proteins in the dataset, adding progression bars, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    seq_input1 = Input(shape=(seq_size, dim), name='seq1')\n",
    "    seq_input2 = Input(shape=(seq_size, dim), name='seq2')\n",
    "    l1=Conv1D(hidden_dim, 3)\n",
    "    #r1=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    r1=Bidirectional(GRU(hidden_dim, return_sequences=True))\n",
    "    l2=Conv1D(hidden_dim, 3)\n",
    "    #r2=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    r2=Bidirectional(GRU(hidden_dim, return_sequences=True))\n",
    "    l3=Conv1D(hidden_dim, 3)\n",
    "    #r3=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    r3=Bidirectional(GRU(hidden_dim, return_sequences=True))\n",
    "    l4=Conv1D(hidden_dim, 3)\n",
    "    #r4=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    r4=Bidirectional(GRU(hidden_dim, return_sequences=True))\n",
    "    l5=Conv1D(hidden_dim, 3)\n",
    "    #r5=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    r5=Bidirectional(GRU(hidden_dim, return_sequences=True))\n",
    "    l6=Conv1D(hidden_dim, 3)\n",
    "    s1=MaxPooling1D(3)(l1(seq_input1))\n",
    "    s1=concatenate([r1(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l2(s1))\n",
    "    s1=concatenate([r2(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l3(s1))\n",
    "    s1=concatenate([r3(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l4(s1))\n",
    "    s1=concatenate([r4(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l5(s1))\n",
    "    s1=concatenate([r5(s1), s1])\n",
    "    s1=l6(s1)\n",
    "    s1=GlobalAveragePooling1D()(s1)\n",
    "    s2=MaxPooling1D(3)(l1(seq_input2))\n",
    "    s2=concatenate([r1(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l2(s2))\n",
    "    s2=concatenate([r2(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l3(s2))\n",
    "    s2=concatenate([r3(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l4(s2))\n",
    "    s2=concatenate([r4(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l5(s2))\n",
    "    s2=concatenate([r5(s2), s2])\n",
    "    s2=l6(s2)\n",
    "    s2=GlobalAveragePooling1D()(s2)\n",
    "    merge_text = multiply([s1, s2])\n",
    "    x = Dense(100, activation='linear')(merge_text)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(int((hidden_dim+7)/2), activation='linear')(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    main_output = Dense(2, activation='softmax')(x)\n",
    "    merge_model = Model(inputs=[seq_input1, seq_input2], outputs=[main_output])\n",
    "    return merge_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This function is the core of the program. It corresponds to the structure of the neural network that is built (and shown below). In principle, since the idea is to predict protein interactions, it takes two protein sequences, and uses a Siamese Network in which each protein sequence goes through a separate number of layers. However, each subnetwork is identical, meaning that the parameters and weights are shared, and updated in the same way. The ouput of each subnetwork is then regrouped, then transformed into a real normalized value, indicating a prediction score for that interaction.\n",
    "- In practice, each layer can be coded into a single line, as shown above. For example, Conv1D(n, m) performs a one-dimensional convolution with n output filters, and a kernel size of m.\n",
    "\n",
    "#### Note\n",
    "- As opposed to image prediction, protein-protein interaction prediction can be more difficult to visualize. In general we use similar architectures than those used in image prediction for example. The design of the architecture, the number of layers, epochs, etc. has an influence on the performance of the network. The exploration of the different hyper-parameters is not easy, but can be automated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/PIPR_Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size1 = 256\n",
    "adam = Adam(lr=0.001, amsgrad=True, epsilon=1e-6)\n",
    "rms = RMSprop(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A certain number of choices are needed for training. \n",
    "- In principle, the training step involves the model making predictions on the training data, and using the error of these predictions to update the model, so that the error is reduced.\n",
    "- The batch size corresponds to the number of samples that are passed through the network together, before updating the internal model parameters. The batch size can take values from 1 (updates are computed after each interaction example) to the number of elements in the training dataset (errors are computed for each example, but the model is updated only after the whole dataset has been passed through the network). Here we have a compromise between those two cases, aiming at balance between efficiency and robustness.\n",
    "- The learning rate corresponds to the step used for the update of model, when minimizing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "#kf = KFold(n_splits=5, shuffle=True)\n",
    "kf = KFold(n_splits=2, shuffle=True)\n",
    "tries = 5\n",
    "cur = 0\n",
    "recalls = []\n",
    "accuracy = []\n",
    "total = []\n",
    "total_truth = []\n",
    "train_test = []\n",
    "for train, test in kf.split(class_labels):\n",
    "    if np.sum(class_labels[train], 0)[0] > 0.8 * len(train) or np.sum(class_labels[train], 0)[0] < 0.2 * len(train):\n",
    "        continue\n",
    "    train_test.append((train, test))\n",
    "    cur += 1\n",
    "    if cur >= tries:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last parameters that are needed for training is the split of the original dataset into training and testing.\n",
    "- In our example, we perform a 5-fold cross-validation. The dataset is divided into 5 block, each block is used once for testing while the others are used for training. This means that 5 separate training phases will be performed, with 5 different training sets. We can look at the training/test set size below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training phases: 5\n",
      "Number of samples in first training set:  8949\n",
      "Number of samples in first test set:  2238\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of training phases:\",len(train_test))\n",
    "dataset1 = train_test[0]\n",
    "print(\"Number of samples in first training set: \",len(dataset1[0]))\n",
    "print(\"Number of samples in first test set: \",len(dataset1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of protein sequence hot-encoding:\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of protein sequence hot-encoding:\\n\",seq_tensor[seq_index1[dataset1[1][0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5593/5593 [==============================] - 100s 18ms/step - loss: 0.6933 - accuracy: 0.4960\n",
      "Epoch 2/2\n",
      "5593/5593 [==============================] - 86s 15ms/step - loss: 0.6930 - accuracy: 0.5171\n",
      "Epoch 1/2\n",
      "5594/5594 [==============================] - 92s 16ms/step - loss: 0.6936 - accuracy: 0.4993\n",
      "Epoch 2/2\n",
      "5594/5594 [==============================] - 84s 15ms/step - loss: 0.6932 - accuracy: 0.4971\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#copy below\n",
    "num_hit = 0.\n",
    "num_total = 0.\n",
    "num_pos = 0.\n",
    "num_true_pos = 0.\n",
    "num_false_pos = 0.\n",
    "num_true_neg = 0.\n",
    "num_false_neg = 0.\n",
    "\n",
    "for train, test in train_test:\n",
    "    merge_model = None\n",
    "    merge_model = build_model()\n",
    "    adam = Adam(lr=0.001, amsgrad=True, epsilon=1e-6)\n",
    "    rms = RMSprop(lr=0.001)\n",
    "    merge_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    merge_model.fit([seq_tensor[seq_index1[train]], seq_tensor[seq_index2[train]]], class_labels[train], batch_size=batch_size1, epochs=n_epochs)\n",
    "    #result1 = merge_model.evaluate([seq_tensor1[test], seq_tensor2[test]], class_labels[test])\n",
    "    pred = merge_model.predict([seq_tensor[seq_index1[test]], seq_tensor[seq_index2[test]]])\n",
    "    for i in range(len(class_labels[test])):\n",
    "        num_total += 1\n",
    "        if np.argmax(class_labels[test][i]) == np.argmax(pred[i]):\n",
    "            num_hit += 1\n",
    "        if class_labels[test][i][0] > 0.:\n",
    "            num_pos += 1.\n",
    "            if pred[i][0] > pred[i][1]:\n",
    "                num_true_pos += 1\n",
    "            else:\n",
    "                num_false_neg += 1\n",
    "        else:\n",
    "            if pred[i][0] > pred[i][1]:\n",
    "                num_false_pos += 1\n",
    "            else:\n",
    "                num_true_neg += 1\n",
    "    '''accuracy = num_hit / num_total\n",
    "    prec = num_true_pos / (num_true_pos + num_false_pos)\n",
    "    recall = num_true_pos / num_pos\n",
    "    spec = num_true_neg / (num_true_neg + num_false_neg)\n",
    "    f1 = 2. * prec * recall / (prec + recall)\n",
    "    mcc = (num_true_pos * num_true_neg - num_false_pos * num_false_neg) / ((num_true_pos + num_true_neg) * (num_true_pos + num_false_neg) * (num_false_pos + num_true_neg) * (num_false_pos + num_false_neg)) ** 0.5\n",
    "    print (accuracy, prec, recall, spec, f1, mcc)\n",
    "    '''\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final step is doing the actual training/testing. Simple performance indicators are computed, such as the specificity, accuracy, recall f1-score and mcc.\n",
    "- The accuracy is shown after each epoch, it should increase progressively. After a certain number of epochs, the performance should stabilize. After 100 epochs, for this network, the performance is quite high (f1-score around 97%).\n",
    "- Note : for reasonable computing time, GPU is required, as well as a fair amount of memory (depending on the dataset size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5180119781889694\n",
      "Precision: 0.5230981383589979\n",
      "Recall: 0.40693724298229933\n",
      "F1-score: 0.45776347546259055\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "accuracy = num_hit / num_total\n",
    "prec = num_true_pos / (num_true_pos + num_false_pos)\n",
    "recall = num_true_pos / num_pos\n",
    "spec = num_true_neg / (num_true_neg + num_false_neg)\n",
    "f1 = 2. * prec * recall / (prec + recall)\n",
    "mcc = (num_true_pos * num_true_neg - num_false_pos * num_false_neg) / ((num_true_pos + num_true_neg) * (num_true_pos + num_false_neg) * (num_false_pos + num_true_neg) * (num_false_pos + num_false_neg)) ** 0.5\n",
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "with open(rst_file, 'w') as fp:\n",
    "    fp.write('acc=' + str(accuracy) + '\\tprec=' + str(prec) + '\\trecall=' + str(recall) + '\\tspec=' + str(spec) + '\\tf1=' + str(f1) + '\\tmcc=' + str(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have successfully trained and tested a Deep Network for Protein-Protein Interaction Prediction with PIPR. The network we are currently working on has a similar architecture, namely a Siamese Network. However, we introduce a certain number of preprocessing steps for the training dataset, as well as another approach in the way the convolution layers are applied, then merged (an illustration of our architecture is shown below). Our model is in its last phase of testing, and will soon be available.\n",
    "![title](images/Imprint_Architecture.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
